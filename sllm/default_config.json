{
    "model": "",
    "backend": "transformers",
    "num_gpus": 1,
    "auto_scaling_config": {
      "metric": "concurrency",
      "target": 1,
      "min_instances": 0,
      "max_instances": 1,
      "keep_alive": 0
    },
    "backend_config": {
      "pretrained_model_name_or_path": "",
      "device_map": "auto",
      "torch_dtype": "float16",
      "hf_model_class": "AutoModelForCausalLM"
    }
  }
