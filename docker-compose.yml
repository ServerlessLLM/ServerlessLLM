# ---------------------------------------------------------------------------- #
#  ServerlessLLM                                                               #
#  Copyright (c) ServerlessLLM Team 2024                                       #
#                                                                              #
#  Licensed under the Apache License, Version 2.0 (the "License");             #
#  you may not use this file except in compliance with the License.            #
#                                                                              #
#  You may obtain a copy of the License at                                     #
#                                                                              #
#                  http://www.apache.org/licenses/LICENSE-2.0                  #
#                                                                              #
#  Unless required by applicable law or agreed to in writing, software         #
#  distributed under the License is distributed on an "AS IS" BASIS,           #
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    #
#  See the License for the specific language governing permissions and         #
#  limitations under the License.                                              #
# ---------------------------------------------------------------------------- #

# Docker Compose for ServerlessLLM HTTP-based Architecture
version: '3.8'

services:
  # Redis for coordination and task queuing
  redis:
    image: redis:7-alpine
    container_name: sllm-redis
    ports:
      - "8008:8008"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - sllm-network

  # Head Node (API Gateway + Management)
  head:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: sllm-head
    environment:
      - MODE=HEAD
      - HEAD_HOST=0.0.0.0
      - HEAD_PORT=8343
      - REDIS_HOST=redis
      - REDIS_PORT=8008
      - STORAGE_PATH=/models
      - LOG_LEVEL=INFO
    ports:
      - "8008:8008"
    volumes:
      - models_data:/models
      - ./logs:/app/logs
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8008/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    networks:
      - sllm-network
    deploy:
      resources:
        reservations:
          memory: 2G
        limits:
          memory: 4G

  # Worker Node 1 (GPU-based inference)
  worker1:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: sllm-worker1
    environment:
      - MODE=WORKER
      - WORKER_HOST=0.0.0.0
      - WORKER_PORT=8000
      - HEAD_NODE_URL=http://head:8343
      - STORAGE_PATH=/models
      - LOG_LEVEL=INFO
      - CUDA_VISIBLE_DEVICES=0
    ports:
      - "8001:8000"
    volumes:
      - models_data:/models
      - ./logs:/app/logs
    depends_on:
      head:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - sllm-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
          memory: 4G
        limits:
          memory: 16G

  # Worker Node 2 (Additional worker - optional)
  worker2:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: sllm-worker2
    environment:
      - MODE=WORKER
      - WORKER_HOST=0.0.0.0
      - WORKER_PORT=8000
      - HEAD_NODE_URL=http://head:8343
      - STORAGE_PATH=/models
      - LOG_LEVEL=INFO
      - CUDA_VISIBLE_DEVICES=1
    ports:
      - "8002:8000"
    volumes:
      - models_data:/models
      - ./logs:/app/logs
    depends_on:
      head:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - sllm-network
    profiles:
      - multi-gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
          memory: 4G
        limits:
          memory: 16G

  # Optional: Redis monitoring with Redis Insight
  redis-insight:
    image: redislabs/redisinsight:latest
    container_name: sllm-redis-insight
    ports:
      - "8001:8001"
    environment:
      - REDIS_HOSTS=local:redis:8008
    depends_on:
      - redis
    networks:
      - sllm-network
    profiles:
      - monitoring

  # Optional: Log aggregation with simple log viewer
  logs:
    image: amir20/dozzle:latest
    container_name: sllm-logs
    ports:
      - "9999:8008"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - sllm-network
    profiles:
      - monitoring

volumes:
  # Shared model storage across all nodes
  models_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./models
  
  # Redis persistent storage
  redis_data:
    driver: local

networks:
  sllm-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
