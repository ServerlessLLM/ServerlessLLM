services:
  # Head Node
  sllm_head:
    build:
      context: ../../
      dockerfile: Dockerfile
    image: seanjiang01/sllm:latest
    container_name: sllm_head
    init: true
    environment:
      - MODEL_FOLDER=${MODEL_FOLDER:-/models}
      - MODE=HEAD
    ports:
      - "6380:6380"    # Redis port
      - "8500:8500"    # ServerlessLLM port
    networks:
      - sllm_network
    command: []

  # Worker Node 0
  sllm_worker_0:
    build:
      context: ../../
      dockerfile: Dockerfile
    image: seanjiang01/sllm:latest
    container_name: sllm_worker_0
    init: true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"] # Assigns GPU 0 to the worker
    environment:
      - WORKER_ID=0
      - STORAGE_PATH=/models
      - MODE=WORKER
    networks:
      - sllm_network
    volumes:
      - ${MODEL_FOLDER:-/models}:/models
    command: ["--mem-pool-size", "96GB", "--registration-required", "true"] # Customize the memory pool size here

networks:
  sllm_network:
    driver: bridge
    name: sllm
