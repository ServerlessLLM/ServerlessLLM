# Benchmark Job Only - Use this when deployments are already running
# Usage: kubectl create -f sllm-benchmark-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  generateName: sllm-benchmark-
  labels:
    app: sllm-benchmark
    kueue.x-k8s.io/queue-name: eidf230ns-user-queue
spec:
  completions: 1
  backoffLimit: 20
  ttlSecondsAfterFinished: 3600
  template:
    metadata:
      labels:
        app: sllm-benchmark
    spec:
      restartPolicy: OnFailure
      initContainers:
        # Wait for pylet head to be ready
        - name: wait-for-pylet-head
          image: curlimages/curl:latest
          command:
            - /bin/sh
            - -c
            - |
              echo "Waiting for pylet-head to be ready..."
              until curl -sf http://pylet-head:8000/workers; do
                echo "Pylet head not ready yet, retrying in 10s..."
                sleep 10
              done
              echo "Pylet head is ready!"
          resources:
            requests:
              cpu: 1
              memory: '1Gi'
            limits:
              cpu: 1
              memory: '1Gi'
        # Wait for the sllm head node to be ready
        - name: wait-for-head
          image: curlimages/curl:latest
          command:
            - /bin/sh
            - -c
            - |
              echo "Waiting for sllm-head to be ready..."
              until curl -sf http://sllm-head:8343/health; do
                echo "Head node not ready yet, retrying in 10s..."
                sleep 10
              done
              echo "Head node is ready!"
          resources:
            requests:
              cpu: 1
              memory: '1Gi'
            limits:
              cpu: 1
              memory: '1Gi'
        # Wait for worker to register
        - name: wait-for-worker
          image: curlimages/curl:latest
          command:
            - /bin/sh
            - -c
            - |
              echo "Waiting for pylet worker to register (60s)..."
              sleep 60
              echo "Worker should be registered now."
          resources:
            requests:
              cpu: 1
              memory: '1Gi'
            limits:
              cpu: 1
              memory: '1Gi'
      containers:
        - name: benchmark
          image: seanjiang01/sllm:latest
          command:
            - /bin/bash
            - -c
            - |
              set -e            
              export NCCL_IB_DISABLE=1
              export LLM_SERVER_URL=http://sllm-head:8343
              
              echo "============================================"
              echo "Starting MoE-CAP Benchmark"
              echo "============================================"
              echo "Server URL: $LLM_SERVER_URL"
              echo "Model: Qwen/Qwen3-30B-A3B"
              echo "============================================"
              
              cat /app/entrypoint.sh

              # Step 1: Deploy the model
              echo ""
              echo "[Step 1/3] Deploying model..."
              sllm deploy --model Qwen/Qwen3-30B-A3B \
                --config /app/examples/moecap/moe_cap_config.json
              
              echo "Model deployment initiated. Waiting for model to be ready..."
              sleep 30
              
              # Step 2: Wait for model to be fully loaded
              echo ""
              echo "[Step 2/3] Checking model status..."
              MAX_RETRIES=60
              RETRY_COUNT=0
              while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
                if curl -sf "$LLM_SERVER_URL/v1/models" | grep -q "Qwen3-30B-A3B"; then
                  echo "Model is ready!"
                  break
                fi
                echo "Model not ready yet, retrying in 30s... ($RETRY_COUNT/$MAX_RETRIES)"
                sleep 30
                RETRY_COUNT=$((RETRY_COUNT + 1))
              done
              
              if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
                echo "ERROR: Model failed to load within timeout"
                exit 1
              fi
              
              # Step 3: Run the evaluation
              echo ""
              echo "[Step 3/3] Running evaluation..."
              python /app/examples/moecap/sllm_profile.py \
                --model_name Qwen/Qwen3-30B-A3B \
                --datasets gsm8k \
                --api-url $LLM_SERVER_URL/v1/chat/completions \
                --backend moe-cap-sglang \
                --num-samples 50 \
                --output_dir /models/moecap_results \
              
              echo ""
              echo "============================================"
              echo "Benchmark completed!"
              echo "Results saved to /models/moecap_results"
              echo "============================================"
              
              # List results
              ls -la /models/moecap_results/
          env:
            - name: LLM_SERVER_URL
              value: "http://sllm-head:8343"
            - name: HF_HOME
              value: "/models/huggingface"
            - name: SLLM_DATABASE_PATH
              value: "/models/.sllm/state.db"
            - name: SGLANG_EXPERT_DISTRIBUTION_RECORDER_DIR
              value: "/models/expert_records"
          volumeMounts:
            - name: model-storage
              mountPath: /models
            - name: shm
              mountPath: /dev/shm
          resources:
            requests:
              cpu: 4
              memory: '16Gi'
            limits:
              cpu: 4
              memory: '16Gi'
      volumes:
        - name: model-storage
          persistentVolumeClaim:
            claimName: model-pvc
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
