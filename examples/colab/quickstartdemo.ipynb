{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n0TpUW4Ek6x"
      },
      "source": [
        "This colab will setup a ray cluster for ServerlessLLM, following the [quickstart](https://serverlessllm.github.io/docs/stable/getting_started/quickstart).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQg8Q2i1FSSe"
      },
      "source": [
        "# Installation\n",
        "Run the following code to first install ServerlessLLM and its dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm-cRIeBFYwV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#This will take a few minutes\n",
        "#In a production environment Conda should be used instead\n",
        "!git clone https://github.com/ServerlessLLM/ServerlessLLM\n",
        "%cd ServerlessLLM\n",
        "!pip install virtualenv\n",
        "!virtualenv sllm; source sllm/bin/activate; pip install -e .; pip install -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ serverless_llm_store; pip install -U torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "!virtualenv sllm-worker; source sllm-worker/bin/activate; pip install -e \".[worker]\"; pip install -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ serverless_llm_store; pip install -U torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxrKYOzuJL-i"
      },
      "source": [
        "# Nodes\n",
        "Now run the following block to start a ray cluster (note there are less workers due to Colab restrictions), the store server and the main server:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEEbwSLfJrAm"
      },
      "outputs": [],
      "source": [
        "!source sllm/bin/activate; nohup ray start --head --port=6379 --num-cpus=2 --num-gpus=0 --resources='{\"control_node\": 1}' --block &\n",
        "!sleep 15\n",
        "!source sllm-worker/bin/activate; nohup ray start --address=0.0.0.0:6379 --num-cpus=2 --num-gpus=1 --resources='{\"worker_node\": 1, \"worker_id_0\": 1}' --block &\n",
        "!sleep 15\n",
        "!source sllm-worker/bin/activate; nohup sllm-store-server &\n",
        "!sleep 10\n",
        "!source sllm/bin/activate; nohup sllm-serve start &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTpPxv24NAWv"
      },
      "source": [
        "# Inference\n",
        "Finally, we can deploy a model, and call the endpoint with an appropriate query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YmZ_PLBWgcD"
      },
      "outputs": [],
      "source": [
        "!source sllm/bin/activate; sllm-cli deploy --model facebook/opt-1.3b --backend transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMhYGMq4K1aa"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "r = requests.post(r\"http://localhost:8343/v1/chat/completions\",\n",
        "  headers = {\n",
        "      \"Content-Type\": \"application/json\"\n",
        "  },\n",
        "  json = {\n",
        "      \"model\": \"facebook/opt-1.3b\",\n",
        "      \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is your name?\"}\n",
        "      ]\n",
        "  }\n",
        ")\n",
        "print(r.json()[\"choices\"][0][\"message\"][\"content\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bQg8Q2i1FSSe"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
