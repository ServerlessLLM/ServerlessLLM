{
  "model": "Qwen/Qwen3-30B-A3B",
  "backend": "vllm_moecap",
  "num_gpus": 4,
  "backend_config": {
    "tensor_parallel_size": 4,
    "max_model_len": 40960,
    "enforce_eager": false,
    "enable_prefix_caching": true,
    "trust_remote_code": true,
    "gpu_memory_utilization": 0.9
  },
  "auto_scaling_config": {
        "metric": "concurrency",
        "target": 10,
        "min_instances": 0,
        "max_instances": 1,
        "keep_alive": 60
  }
}