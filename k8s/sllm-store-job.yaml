apiVersion: batch/v1
kind: Job
metadata:
  name: sllm-store-worker
  labels:
    kueue.x-k8s.io/queue-name: <YOUR_NAMESPACE>-user-queue  # REQUIRED: Replace with your namespace
spec:
  template:
    metadata:
      labels:
        app: sllm-store
    spec:
      restartPolicy: Never
      containers:
      - name: sllm-store
        image: <YOUR_REGISTRY>/sllm-store:latest  # REQUIRED: Replace with your Docker registry and image name

        # Resource requests and limits (REQUIRED by EIDF)
        resources:
          requests:
            cpu: "<CPU_REQUEST>"           # REQUIRED: e.g., "4" or "4000m"
            memory: "<MEMORY_REQUEST>"     # REQUIRED: e.g., "16Gi"
            nvidia.com/gpu: <GPU_COUNT>    # REQUIRED: Number of GPUs, e.g., 1
          limits:
            cpu: "<CPU_LIMIT>"             # REQUIRED: e.g., "4" or "4000m"
            memory: "<MEMORY_LIMIT>"       # REQUIRED: e.g., "16Gi"
            nvidia.com/gpu: <GPU_COUNT>    # REQUIRED: Must match requests

        # sllm-store command with configurable options
        command: ["sllm-store", "start"]
        args:
        - "--storage-path"
        - "/models"
        - "--mem-pool-size"
        - "<MEM_POOL_SIZE>"               # REQUIRED: e.g., "4GB", "8GB", "16GB" - should be >= your largest model size
        # Optional: Add more args like:
        # - "--port"
        # - "8073"

        # Volume mounts for model storage
        volumeMounts:
        - name: model-storage
          mountPath: /models

        # Optional: Environment variables
        env:
        - name: PYTHONUNBUFFERED
          value: "1"
        # Add custom env vars if needed:
        # - name: LOG_LEVEL
        #   value: "INFO"

      # Volume configuration (REQUIRED)
      volumes:
      - name: model-storage
        # OPTION 1: Use PersistentVolumeClaim (recommended for production)
        persistentVolumeClaim:
          claimName: <YOUR_PVC_NAME>      # REQUIRED if using PVC: Replace with your PVC name

        # OPTION 2: Use hostPath (for accessing node-local storage)
        # hostPath:
        #   path: <HOST_PATH>              # REQUIRED if using hostPath: e.g., "/data/models"
        #   type: Directory

        # OPTION 3: Use emptyDir (temporary storage, data lost when pod terminates)
        # emptyDir: {}

      # Optional: Node selector to specify GPU type
      # nodeSelector:
      #   nvidia.com/gpu.product: '<GPU_PRODUCT_NAME>'  # e.g., 'NVIDIA-A100-SXM4-40GB' or 'NVIDIA-A100-SXM4-40GB-MIG-1g.5gb'

      # Optional: Tolerations for node taints
      # tolerations:
      # - key: "nvidia.com/gpu"
      #   operator: "Exists"
      #   effect: "NoSchedule"
