apiVersion: v1
kind: ConfigMap
metadata:
  name: benchmark-scripts-full
  namespace: <YOUR_NAMESPACE>  # REQUIRED: Replace with your namespace
data:
  # Download and run benchmark script
  # This downloads scripts from GitHub at runtime to avoid embedding large files
  run-benchmark.sh: |
    #!/bin/bash
    set -e

    # Configuration
    MODEL_NAME="${MODEL_NAME:-facebook/opt-1.3b}"
    NUM_REPLICAS="${NUM_REPLICAS:-10}"
    MEM_POOL_SIZE="${MEM_POOL_SIZE:-8GB}"
    STORAGE_PATH="${STORAGE_PATH:-/models}"
    RESULTS_PATH="${RESULTS_PATH:-/results}"
    BENCHMARK_TYPE="${BENCHMARK_TYPE:-random}"
    GENERATE_PLOTS="${GENERATE_PLOTS:-false}"
    KEEP_ALIVE="${KEEP_ALIVE:-false}"
    BRANCH="${BRANCH:-claude/serverlessllm-docker-setup-01SxEhpoNSDpRQp1iKqWfyRS}"

    LOG_FILE="${RESULTS_PATH}/benchmark.log"
    SUMMARY_FILE="${RESULTS_PATH}/summary.txt"

    # Create directories first
    mkdir -p "$RESULTS_PATH" /tmp/benchmarks
    cd /tmp/benchmarks

    # Define log function after directories exist
    log() { echo "[$(date +'%Y-%m-%d %H:%M:%S')] $*" | tee -a "$LOG_FILE"; }

    log "=== ServerlessLLM Automated Benchmark ==="
    log "Model: $MODEL_NAME"
    log "Replicas: $NUM_REPLICAS"
    log "Memory Pool: $MEM_POOL_SIZE"
    log "Storage: $STORAGE_PATH"
    log "Branch: $BRANCH"
    log ""

    # Install dependencies
    log "Installing dependencies..."
    pip install -q seaborn matplotlib pandas sentencepiece 2>&1 | tee -a "$LOG_FILE"

    # Install curl if not available
    if ! command -v curl &> /dev/null; then
        log "Installing curl..."
        apt-get update -qq && apt-get install -y -qq curl 2>&1 | tee -a "$LOG_FILE"
    fi

    # Activate conda environment (required for official serverlessllm/sllm image)
    log "Activating conda environment..."
    source /opt/conda/etc/profile.d/conda.sh
    conda activate worker 2>&1 | tee -a "$LOG_FILE" || conda activate head 2>&1 | tee -a "$LOG_FILE"

    # Download benchmark scripts from GitHub
    log "Downloading benchmark scripts..."
    curl -sL "https://raw.githubusercontent.com/ServerlessLLM/ServerlessLLM/${BRANCH}/benchmarks/download_models.py" -o download_models.py
    curl -sL "https://raw.githubusercontent.com/ServerlessLLM/ServerlessLLM/${BRANCH}/benchmarks/test_loading.py" -o test_loading.py
    curl -sL "https://raw.githubusercontent.com/ServerlessLLM/ServerlessLLM/${BRANCH}/benchmarks/benchmark_utils.py" -o benchmark_utils.py
    curl -sL "https://raw.githubusercontent.com/ServerlessLLM/ServerlessLLM/${BRANCH}/benchmarks/generate_report.py" -o generate_report.py
    chmod +x generate_report.py

    # Start sllm-store (match official benchmark setup)
    log "Starting sllm-store server..."
    sllm-store start \
        --storage-path "$STORAGE_PATH" \
        --mem-pool-size "$MEM_POOL_SIZE" \
        --chunk-size 16MB \
        --num-thread 4 \
        > "${RESULTS_PATH}/sllm-store.log" 2>&1 &

    SLLM_STORE_PID=$!
    log "sllm-store started (PID: $SLLM_STORE_PID)"

    # Wait for ready
    log "Waiting for sllm-store..."
    sleep 10

    if ! kill -0 $SLLM_STORE_PID 2>/dev/null; then
        log "ERROR: sllm-store failed to start"
        cat "${RESULTS_PATH}/sllm-store.log"
        exit 1
    fi
    log "sllm-store ready"

    # Cleanup on exit
    cleanup() {
        log "Cleaning up..."
        kill $SLLM_STORE_PID 2>/dev/null || true
        wait $SLLM_STORE_PID 2>/dev/null || true
    }
    trap cleanup EXIT INT TERM

    # Run benchmarks
    log "Starting benchmarks..."
    rm -rf "${STORAGE_PATH:?}"/* || true

    for MODEL_FORMAT in safetensors sllm; do
        log "=== Testing $MODEL_FORMAT format ==="

        log "Downloading $NUM_REPLICAS replicas..."
        python3 download_models.py \
            --model-name "$MODEL_NAME" \
            --save-format "$MODEL_FORMAT" \
            --save-dir "$STORAGE_PATH" \
            --num-replicas "$NUM_REPLICAS" \
            2>&1 | tee -a "$LOG_FILE"

        log "Running benchmark..."
        python3 test_loading.py \
            --model-name "$MODEL_NAME" \
            --model-format "$MODEL_FORMAT" \
            --model-dir "$STORAGE_PATH" \
            --num-replicas "$NUM_REPLICAS" \
            --benchmark-type "$BENCHMARK_TYPE" \
            --output-dir "$RESULTS_PATH" \
            2>&1 | tee -a "$LOG_FILE"

        log "Cleaning storage..."
        rm -rf "${STORAGE_PATH:?}"/* || true
    done

    # Generate report
    log "Generating report..."
    python3 generate_report.py \
        --model-name "$MODEL_NAME" \
        --num-replicas "$NUM_REPLICAS" \
        --benchmark-type "$BENCHMARK_TYPE" \
        --results-dir "$RESULTS_PATH" \
        --output-file "$SUMMARY_FILE" \
        --generate-plots "$GENERATE_PLOTS" \
        2>&1 | tee -a "$LOG_FILE"

    log ""
    log "=== Benchmark Summary ==="
    cat "$SUMMARY_FILE" | tee -a "$LOG_FILE"
    log ""
    log "=== Benchmark Complete ==="
    log "Results: $RESULTS_PATH"

    if [ "$KEEP_ALIVE" = "true" ]; then
        log "KEEP_ALIVE=true, keeping container running..."
        tail -f /dev/null
    fi
