From e1370cc96a915419d70597a8f500b0c8bdc33b2f Mon Sep 17 00:00:00 2001
From: FionaY728 <utachan899@gmail.com>
Date: Wed, 25 Jun 2025 14:49:34 +0100
Subject: [PATCH] feat: add ServerlessLLM loader and load_format support

---
 python/sglang/srt/configs/load_config.py      |  1 +
 python/sglang/srt/model_loader/__init__.py    | 43 +++++++++++-
 python/sglang/srt/model_loader/sllm_loader.py | 70 +++++++++++++++++++
 3 files changed, 113 insertions(+), 1 deletion(-)
 create mode 100644 python/sglang/srt/model_loader/sllm_loader.py

diff --git a/python/sglang/srt/configs/load_config.py b/python/sglang/srt/configs/load_config.py
index be9a40b4..2c65a6ca 100644
--- a/python/sglang/srt/configs/load_config.py
+++ b/python/sglang/srt/configs/load_config.py
@@ -23,6 +23,7 @@ class LoadFormat(str, enum.Enum):
     LAYERED = "layered"
     JAX = "jax"
     REMOTE = "remote"
+    SERVERLESS_LLM = "serverless_llm"
 
 
 @dataclass
diff --git a/python/sglang/srt/model_loader/__init__.py b/python/sglang/srt/model_loader/__init__.py
index fa2386e3..5e9af887 100644
--- a/python/sglang/srt/model_loader/__init__.py
+++ b/python/sglang/srt/model_loader/__init__.py
@@ -1,10 +1,43 @@
 # Adapted from https://github.com/vllm-project/vllm/blob/v0.6.4.post1/vllm/model_executor/model_loader/__init__.py
 
+# from torch import nn
+
+# from sglang.srt.configs.device_config import DeviceConfig
+# from sglang.srt.configs.load_config import LoadConfig
+# from sglang.srt.configs.model_config import ModelConfig
+# from sglang.srt.model_loader.loader import BaseModelLoader, get_model_loader
+# from sglang.srt.model_loader.utils import (
+#     get_architecture_class_name,
+#     get_model_architecture,
+# )
+
+
+# def get_model(
+#     *,
+#     model_config: ModelConfig,
+#     load_config: LoadConfig,
+#     device_config: DeviceConfig,
+# ) -> nn.Module:
+#     loader = get_model_loader(load_config)
+#     return loader.load_model(
+#         model_config=model_config,
+#         device_config=device_config,
+#     )
+
+
+# __all__ = [
+#     "get_model",
+#     "get_model_loader",
+#     "BaseModelLoader",
+#     "get_architecture_class_name",
+#     "get_model_architecture",
+# ]
 from torch import nn
 
 from sglang.srt.configs.device_config import DeviceConfig
-from sglang.srt.configs.load_config import LoadConfig
+from sglang.srt.configs.load_config import LoadConfig, LoadFormat
 from sglang.srt.configs.model_config import ModelConfig
+from sglang.srt.model_loader.sllm_loader import ServerlessLLMLoader
 from sglang.srt.model_loader.loader import BaseModelLoader, get_model_loader
 from sglang.srt.model_loader.utils import (
     get_architecture_class_name,
@@ -18,6 +51,13 @@ def get_model(
     load_config: LoadConfig,
     device_config: DeviceConfig,
 ) -> nn.Module:
+   
+    if load_config.load_format == LoadFormat.SERVERLESS_LLM:
+        return ServerlessLLMLoader(load_config).load_model(
+            model_config=model_config,
+            device_config=device_config,
+        )
+
     loader = get_model_loader(load_config)
     return loader.load_model(
         model_config=model_config,
@@ -32,3 +72,4 @@ __all__ = [
     "get_architecture_class_name",
     "get_model_architecture",
 ]
+
diff --git a/python/sglang/srt/model_loader/sllm_loader.py b/python/sglang/srt/model_loader/sllm_loader.py
new file mode 100644
index 00000000..d867fb9d
--- /dev/null
+++ b/python/sglang/srt/model_loader/sllm_loader.py
@@ -0,0 +1,70 @@
+# sglang/srt/model_loader/sllm_loader.py
+
+import os
+import torch
+from torch import nn
+
+from sglang.srt.configs.load_config import LoadConfig
+from sglang.srt.configs.model_config import ModelConfig
+from sglang.srt.configs.device_config import DeviceConfig
+from sglang.srt.model_loader.loader import BaseModelLoader, _initialize_model
+from sglang.srt.model_loader.utils import set_default_torch_dtype
+from sllm_store.torch import load_dict as sllm_load_dict
+
+
+os.environ["SGLANG_TENSOR_MODEL_PARALLEL_SIZE"] = "1"
+os.environ["SGLANG_PIPELINE_MODEL_PARALLEL_SIZE"] = "1"
+
+class ServerlessLLMLoader(BaseModelLoader):
+
+    def __init__(self, load_config: LoadConfig):
+        super().__init__(load_config)
+
+    def download_model(self, model_config: ModelConfig) -> None:
+    
+        pass
+
+    def load_model(
+        self,
+        *,
+        model_config: ModelConfig,
+        device_config: DeviceConfig,
+    ) -> nn.Module:
+       
+        try:
+            hf_cfg = model_config.hf_config
+            if hasattr(hf_cfg, "num_attention_heads") and not hasattr(hf_cfg, "num_key_value_heads"):
+                hf_cfg.num_key_value_heads = hf_cfg.num_attention_heads
+        except Exception:
+            pass
+
+        target_device = torch.device(device_config.device)
+        with set_default_torch_dtype(model_config.dtype):
+            with target_device:
+                model = _initialize_model(model_config, self.load_config)
+        model = model.eval()
+
+
+        storage_path = os.getenv(
+            "STORAGE_PATH",
+            self.load_config.download_dir or "./models"
+        )
+   
+        device_id = torch.cuda.current_device() if target_device.type == "cuda" else -1
+      
+        device_map = {"": device_id}
+
+        state_dict = sllm_load_dict(
+            model_path=model_config.model_path,
+            device_map=device_map,
+            storage_path=storage_path,
+        )
+
+        model.load_state_dict(state_dict, strict=False)
+
+     
+        for name, buf in model.named_buffers():
+            if name in state_dict:
+                buf.data.copy_(state_dict[name].to(target_device))
+
+        return model
-- 
2.34.1

