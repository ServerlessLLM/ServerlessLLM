diff --git a/vllm/config.py b/vllm/config.py
index db35c848b..2c39d9a6b 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -1586,6 +1586,7 @@ class LoadFormat(str, enum.Enum):
     RUNAI_STREAMER = "runai_streamer"
     RUNAI_STREAMER_SHARDED = "runai_streamer_sharded"
     FASTSAFETENSORS = "fastsafetensors"
+    SERVERLESS_LLM = "serverless_llm"
 
 
 @config
diff --git a/vllm/executor/executor_base.py b/vllm/executor/executor_base.py
index 40ca1d299..5771b5d24 100644
--- a/vllm/executor/executor_base.py
+++ b/vllm/executor/executor_base.py
@@ -144,7 +144,18 @@ class ExecutorBase(ABC):
     def stop_remote_worker_execution_loop(self) -> None:
         """Releases parallel workers from model loop."""
         return
-
+    
+    def save_serverless_llm_state(
+        self,
+        path: str,
+        pattern: Optional[str] = None,
+        max_size: Optional[int] = None,
+        ) -> None:
+        
+        self.driver_worker.save_serverless_llm_state(
+            path=path, pattern=pattern, max_size=max_size
+        )
+        
     def add_lora(self, lora_request: LoRARequest) -> bool:
         assert lora_request.lora_int_id > 0, "lora_id must be greater than 0."
         return all(self.collective_rpc("add_lora", args=(lora_request, )))
diff --git a/vllm/executor/mp_distributed_executor.py b/vllm/executor/mp_distributed_executor.py
index d1f8c36fb..0d6d94c57 100644
--- a/vllm/executor/mp_distributed_executor.py
+++ b/vllm/executor/mp_distributed_executor.py
@@ -132,6 +132,17 @@ class MultiprocessingDistributedExecutor(DistributedExecutorBase):
         if (worker_monitor := getattr(self, "worker_monitor",
                                       None)) is not None:
             worker_monitor.close()
+    
+    def save_serverless_llm_state(
+        self,
+        path: str,
+        pattern: Optional[str] = None,
+        max_size: Optional[int] = None,
+        ) -> None:
+        self._run_workers("save_serverless_llm_state",
+                         path=path,
+                         pattern=pattern,
+                         max_size=max_size)
 
     def _driver_execute_model(
         self, execute_model_req: Optional[ExecuteModelRequest]
diff --git a/vllm/executor/ray_distributed_executor.py b/vllm/executor/ray_distributed_executor.py
index 8e67c7a41..01dc4b9cc 100644
--- a/vllm/executor/ray_distributed_executor.py
+++ b/vllm/executor/ray_distributed_executor.py
@@ -427,6 +427,17 @@ class RayDistributedExecutor(DistributedExecutorBase):
                 self.tp_driver_workers.append(worker)
             else:
                 self.non_driver_workers.append(worker)
+    
+    def save_serverless_llm_state(
+        self,
+        path: str,
+        pattern: Optional[str] = None,
+        max_size: Optional[int] = None,
+        ) -> None:
+        self._run_workers("save_serverless_llm_state",
+                         path=path,
+                         pattern=pattern,
+                         max_size=max_size)
 
     def _driver_execute_model(
         self, execute_model_req: Optional[ExecuteModelRequest]
diff --git a/vllm/model_executor/model_loader/__init__.py b/vllm/model_executor/model_loader/__init__.py
index a443a652d..c1869a54a 100644
--- a/vllm/model_executor/model_loader/__init__.py
+++ b/vllm/model_executor/model_loader/__init__.py
@@ -11,6 +11,7 @@ from vllm.model_executor.model_loader.bitsandbytes_loader import (
 from vllm.model_executor.model_loader.default_loader import DefaultModelLoader
 from vllm.model_executor.model_loader.dummy_loader import DummyModelLoader
 from vllm.model_executor.model_loader.gguf_loader import GGUFModelLoader
+from vllm.model_executor.model_loader.sllm_loader import ServerlessLLMLoader
 from vllm.model_executor.model_loader.runai_streamer_loader import (
     RunaiModelStreamerLoader)
 from vllm.model_executor.model_loader.sharded_state_loader import (
@@ -45,6 +46,9 @@ def get_model_loader(load_config: LoadConfig) -> BaseModelLoader:
 
     if load_config.load_format == LoadFormat.RUNAI_STREAMER_SHARDED:
         return ShardedStateLoader(load_config, runai_model_streamer=True)
+    
+    if load_config.load_format == LoadFormat.SERVERLESS_LLM:
+        return ServerlessLLMLoader(load_config)
 
     return DefaultModelLoader(load_config)
 
diff --git a/vllm/model_executor/model_loader/sllm_loader.py b/vllm/model_executor/model_loader/sllm_loader.py
new file mode 100644
index 000000000..baea12c06
--- /dev/null
+++ b/vllm/model_executor/model_loader/sllm_loader.py
@@ -0,0 +1,146 @@
+import torch
+from torch import nn
+import collections
+import os
+import gc
+from typing import Dict, Optional
+from vllm.config import LoadConfig, VllmConfig, ModelConfig
+from vllm.model_executor.model_loader.base_loader import BaseModelLoader
+from vllm.model_executor.model_loader.utils import (
+    initialize_model, set_default_torch_dtype)
+
+class ServerlessLLMLoader(BaseModelLoader):
+    # DEFAULT_PATTERN = "model-rank-{rank}-part-{part}.safetensors"
+
+    def __init__(self, load_config: LoadConfig):
+        super().__init__(load_config)
+        extra_config = ({} if load_config.model_loader_extra_config is None
+                        else load_config.model_loader_extra_config.copy())
+        # self.pattern = extra_config.pop("pattern", self.DEFAULT_PATTERN)
+        if extra_config:
+            raise ValueError(f"Unexpected extra config keys for load format "
+                             f"{load_config.load_format}: "
+                             f"{load_config.model_loader_extra_config.keys()}")
+
+    @staticmethod
+    def _filter_subtensors(
+            tensors: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
+        """
+        Filter out all tensors that share the same memory or a subset of the
+        memory of another tensor.
+        """
+        same_storage_groups = collections.defaultdict(list)
+        for key, tensor in tensors.items():
+            if tensor.numel():
+                ptr = tensor.untyped_storage().data_ptr()
+                same_storage_groups[tensor.device, ptr].append((key, tensor))
+
+        def get_end_ptr(tensor: torch.Tensor) -> int:
+            return tensor.view(-1)[-1].data_ptr() + tensor.element_size()
+
+        result = {}
+        for group in same_storage_groups.values():
+            for k, t in group:
+                a, b = t.data_ptr(), get_end_ptr(t)
+                for k2, t2 in group:
+                    if not t2.is_contiguous():
+                        continue
+                    a2, b2 = t2.data_ptr(), get_end_ptr(t2)
+                    if a < a2 or b2 < b:
+                        continue
+                    if a2 < a or b < b2 or not t.is_contiguous():
+                        break  # t2 covers strictly more memory than t.
+                    if k2 > k:
+                        # Same tensors, keep the one with the longer key.
+                        break
+                else:
+                    result[k] = t
+        return result
+        
+    def load_model(self, *, vllm_config: VllmConfig, **kwargs) -> nn.Module:
+        from sllm_store.torch import load_dict
+        from vllm.distributed import get_tensor_model_parallel_rank
+        
+        assert os.path.isdir(vllm_config.model_config.model)
+        
+        rank = get_tensor_model_parallel_rank()
+
+        local_model_path = vllm_config.model_config.model
+        local_model_path = os.path.join(local_model_path, f"rank_{rank}")
+
+        def remove_prefix(path, prefix):
+            # Normalize the paths to ensure consistency across different platforms
+            path = os.path.normpath(path)
+            prefix = os.path.normpath(prefix)
+            
+            # Check if the path starts with the prefix
+            if path.startswith(prefix):
+                # Return the path without the prefix
+                return path[len(prefix):].lstrip(os.sep)
+            
+            # Return the original path if the prefix doesn't exist
+            return path
+        
+        # vLLM needs a local model path to read model config but
+        # ServerlessLLM Store requires a global model path as the model ID
+        storage_path = os.getenv("STORAGE_PATH", "./models")
+        model_path = remove_prefix(local_model_path, storage_path)
+        
+        with set_default_torch_dtype(vllm_config.model_config.dtype):
+            # with torch.device(device_config.device):
+            with torch.device("cpu"):
+                model = initialize_model(vllm_config=vllm_config)
+                model = model.eval()
+            # set all parameters to meta device
+            state_dict = self._filter_subtensors(model.state_dict())
+            key_list = list(state_dict.keys())
+            
+            for key, param in model.named_parameters(recurse=True):
+                if key in key_list:
+                    param.data = torch.empty(1, device="cuda")
+            gc.collect()
+            
+            device_id = torch.cuda.current_device()
+            device_map = {"": device_id}
+            # Note: storage path is already included in the local model path
+            sllm_state_dict = load_dict(model_path, device_map)
+            
+            for key, param in model.named_parameters(recurse=True):
+                if key in key_list:
+                    tensor = sllm_state_dict[key]
+                    param.data = tensor
+                    state_dict.pop(key)
+            if state_dict:
+                raise ValueError(
+                    f"Missing keys {tuple(state_dict)} in loaded state!")
+            
+        return model
+    
+    def download_model(self, model_config: ModelConfig) -> None:
+        pass
+
+    def load_weights(self, model, model_config):
+        pass
+
+    @staticmethod
+    def save_model(
+        model: torch.nn.Module,
+        path: str,
+        pattern: Optional[str] = None,
+        max_size: Optional[int] = None,
+    ) -> None:
+        from vllm.distributed import get_tensor_model_parallel_rank
+        from sllm_store.torch import save_dict
+        
+        rank = get_tensor_model_parallel_rank()
+        state_dict = ServerlessLLMLoader._filter_subtensors(model.state_dict())
+        
+        # move all tensors to CPU
+        for key, tensor in state_dict.items():
+            state_dict[key] = tensor.cpu().contiguous()
+
+        save_path = os.path.join(path, f"rank_{rank}")
+        if not os.path.exists(save_path):
+            os.makedirs(save_path)
+            
+        save_dict(state_dict, save_path)
\ No newline at end of file
diff --git a/vllm/v1/engine/core.py b/vllm/v1/engine/core.py
index 740ba60fe..1071cb71f 100644
--- a/vllm/v1/engine/core.py
+++ b/vllm/v1/engine/core.py
@@ -331,6 +331,16 @@ class EngineCore:
         self.model_executor.save_sharded_state(path=path,
                                                pattern=pattern,
                                                max_size=max_size)
+    
+    def save_serverless_llm_state(
+        self,
+        path: str,
+        pattern: Optional[str] = None,
+        max_size: Optional[int] = None,
+    ) -> None:
+        self.model_executor.save_serverless_llm_state(path=path,
+                                                      pattern=pattern,
+                                                      max_size=max_size)
 
     def collective_rpc(self,
                        method: Union[str, Callable[..., _R]],
diff --git a/vllm/v1/engine/core_client.py b/vllm/v1/engine/core_client.py
index 0d52bc9a6..95adf9af8 100644
--- a/vllm/v1/engine/core_client.py
+++ b/vllm/v1/engine/core_client.py
@@ -256,6 +256,14 @@ class InprocClient(EngineCoreClient):
                            pattern: Optional[str] = None,
                            max_size: Optional[int] = None) -> None:
         self.engine_core.save_sharded_state(path, pattern, max_size)
+    
+    def save_serverless_llm_state(
+        self,
+        path: str,
+        pattern: Optional[str] = None,
+        max_size: Optional[int] = None,
+    ) -> None:
+        self.engine_core.save_serverless_llm_state(path, pattern, max_size)
 
     def collective_rpc(self,
                        method: Union[str, Callable[..., _R]],
@@ -724,6 +732,14 @@ class SyncMPClient(MPClient):
                            pattern: Optional[str] = None,
                            max_size: Optional[int] = None) -> None:
         self.call_utility("save_sharded_state", path, pattern, max_size)
+    
+    def save_serverless_llm_state(
+        self,
+        path: str,
+        pattern: Optional[str] = None,
+        max_size: Optional[int] = None,
+        ) -> None:
+        self.call_utility("save_serverless_llm_state", path, pattern, max_size)
 
 
 class AsyncMPClient(MPClient):
@@ -906,6 +922,15 @@ class AsyncMPClient(MPClient):
         await self.call_utility_async("save_sharded_state", path, pattern,
                                       max_size)
 
+    async def save_serverless_llm_state_async(
+                                        self,
+                                        path: str,
+                                        pattern: Optional[str] = None,
+                                        max_size: Optional[int] = None,
+                                        ) -> None:
+        await self.call_utility_async("save_serverless_llm_state", path, pattern, 
+                                      max_size)
+
     async def collective_rpc_async(
             self,
             method: Union[str, Callable[..., _R]],
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 910c0e80b..d13645e3b 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -1560,6 +1560,20 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             tensorizer_config=tensorizer_config,
         )
 
+    def save_serverless_llm_state(
+        self,
+        path: str,
+        pattern: Optional[str] = None,
+        max_size: Optional[int] = None,
+    ) -> None:
+        from vllm.model_executor.model_loader import ServerlessLLMLoader
+        ServerlessLLMLoader.save_model(
+            self.model,
+            path,
+            pattern=pattern,
+            max_size=max_size,
+        )
+
     def _get_prompt_logprobs_dict(
         self,
         hidden_states: torch.Tensor,
diff --git a/vllm/v1/worker/gpu_worker.py b/vllm/v1/worker/gpu_worker.py
index bce5cbb5f..55d65c7d3 100644
--- a/vllm/v1/worker/gpu_worker.py
+++ b/vllm/v1/worker/gpu_worker.py
@@ -325,6 +325,18 @@ class Worker(WorkerBase):
             pattern=pattern,
             max_size=max_size,
         )
+    
+    def save_serverless_llm_state(
+        self,
+        path: str,
+        pattern: Optional[str] = None,
+        max_size: Optional[int] = None,
+    ) -> None:
+        self.model_runner.save_serverless_llm_state(
+            path,
+            pattern=pattern,
+            max_size=max_size,
+        )
 
     def save_tensorized_model(
         self,
diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py
index 8c968faa7..932c943ab 100644
--- a/vllm/worker/model_runner.py
+++ b/vllm/worker/model_runner.py
@@ -1236,6 +1236,20 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):
             pattern=pattern,
             max_size=max_size,
         )
+    
+    def save_serverless_llm_state(
+        self,
+        path: str,
+        pattern: Optional[str] = None,
+        max_size: Optional[int] = None,
+    ) -> None:
+        from vllm.model_executor.model_loader import ServerlessLLMLoader
+        ServerlessLLMLoader.save_model(
+            self.model,
+            path,
+            pattern=pattern,
+            max_size=max_size,
+        )
 
     def save_tensorized_model(
         self,
diff --git a/vllm/worker/worker.py b/vllm/worker/worker.py
index 6e45b8423..d87e5e9c6 100644
--- a/vllm/worker/worker.py
+++ b/vllm/worker/worker.py
@@ -218,6 +218,18 @@ class Worker(LocalOrDistributedWorkerBase):
             max_size=max_size,
         )
 
+    def save_serverless_llm_state(
+        self,
+        path: str,
+        pattern: Optional[str] = None,
+        max_size: Optional[int] = None,
+    ) -> None:
+        self.model_runner.save_serverless_llm_state(
+            path,
+            pattern=pattern,
+            max_size=max_size,
+        )
+
     def save_tensorized_model(
         self,
         tensorizer_config: TensorizerConfig,
