name: Backend Tests

on:
  pull_request:
    types: [opened, synchronize, labeled]
    branches:
      - main
    paths:
      - 'sllm/serve/backends/**'

jobs:
    backend_tests:
        runs-on: self-hosted
        if: contains(github.event.pull_request.labels.*.name, 'ready-for-gpu-testing')
        container:
          image: nvcr.io/nvidia/cuda:12.6.1-cudnn-devel-ubuntu22.04
          options: --gpus all
        steps:
            - name: Checkout code
              uses: actions/checkout@v4

            - name: Set up Python
              uses: actions/setup-python@v5
              with:
                  python-version: '3.10'

            - name: Install dependencies
              run: |
                  apt-get update && apt-get install -y ca-certificates git
                  python -m pip install --upgrade pip
                  pip install -r requirements.txt
                  pip install -r requirements-worker.txt
                  pip install -r requirements-test.txt

            - name: Install ServerlessLLM
              run: |
                  pip install .
                  cd sllm_store && pip install . && ./vllm_patch/patch.sh

            - name: Run tests
              run: |
                  pytest tests/backend_test