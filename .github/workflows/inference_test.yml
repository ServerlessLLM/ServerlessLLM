Name: Test Store and Inference 

on: 
  pull_request: 
    branches:
      - main
    paths:
      - 'sllm/serve/**'
      - 'sllm/cli/**'
      - 'tests/inference_test/**'
      - 'docker-build.sh'

jobs: 
  inference_store_tests:
    runs-on: self-hosted
    if: contains(github.event.pull_request.labels.*.name, 'ready-for-gpu-testing')
    container:
      image: pytorch/pytorch:2.3.0-cuda12.1-cudnn8-devel
      options: --gpus all
    steps:
      - uses: actions/checkout@v4

      - name: Setup Environment and Start Services
        run: |
          chmod +x entrypoint.sh
          ./entrypoint.sh
      
      - name: Initialize conda and paths
        run: |
          chmod +x ./tests/inference_test/inference_conda.sh
          source ./tests/inference_test/inference_conda.sh

      - name: Test sllm-cli deploy
        run: |
          source ./tests/inference_test/inference_conda.sh
          python ./tests/inference_test/store_test.py
    
      - name: Test inference
        if: always()
        run: |
          source ./tests/inference_test/inference_conda.sh
          python ./tests/inference_test/inference_test.py

      - name: Check results 
        if: always()
        run: |
          if [ -f failed_models.json ]; then
            echo "::error::Tests failed - see above for details"
            exit 1
          fi

      - name: Cleanup
        if: always()
        run: |
          chmod +x ./tests/inference_test/cleanup.sh
          ./test/inference_test/cleanup.sh
