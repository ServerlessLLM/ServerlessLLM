name: Test Store and Inference

on:
  pull_request:
    branches:
      - main
    paths:
      - 'sllm/serve/**'
      - 'sllm/cli/**'
      - 'tests/inference_test/**'
      - 'docker-build.sh'

jobs:
  inference_store_tests:
    runs-on: self-hosted
    if: contains(github.event.pull_request.labels.*.name, 'ready-for-gpu-testing')
    container:
      image: pytorch/pytorch:2.3.0-cuda12.1-cudnn8-devel
      options: --gpus all
    defaults:
      run:
          shell: bash -l {0}

    steps:
      - uses: actions/checkout@v4
      - uses: conda-incubator/setup-miniconda@v3
        with:
          python-version: 3.10
          activate-environment: sllm
          miniconda-version: "latest"

      - name: Install Dependencies
        run: |
          apt-get update
          apt-get install -y patch git npm wget nodejs


      - name: Import Python libraries
        run: |
          pip install serverless-llm
          pip install serverless-llm-store
          pip install ray vllm

          echo "Creating models directory..."
          mkdir -p models
          echo "Models directory created at $(pwd)/models"

          echo "Setting up environment variables..."
          echo "MODEL_FOLDER=$(pwd)/models" >> $GITHUB_ENV
          echo "LLM_SERVER_URL=http://127.0.0.1:8343/" >> $GITHUB_ENV
          echo "MODEL_FOLDER is set to: $MODEL_FOLDER"
          echo "LLM_SERVER_URL is set to: $LLM_SERVER_URL"

      - name: Setup Environment and Start Services
        env:
          NODE_TYPE: "HEAD"
        run: |
          chmod +x entrypoint.sh
          ./entrypoint.sh

      - name: Test sllm store
        run: |
          cd ./tests/inference_test
          python store_test.py

      - name: Test sllm inference
        if: always()
        run: |
          cd ./tests/inference_test
          python inference_test.py

      - name: Check results
        if: always()
        run: |
          if [ -f failed_models.json ]; then
            echo "::error::Tests failed - see above for details"
            exit 1
          fi
